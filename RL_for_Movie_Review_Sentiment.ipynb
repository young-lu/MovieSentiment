{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_for_Movie_Review_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9jeO33X-Rudc",
        "2yM2pHz2xfXF",
        "wb1fCqlMxvII",
        "d7Rv8Wdlwvzh",
        "bQCcK5Xk_GdM",
        "KJ6VJNc18lEY",
        "RjdSfRXDdAlu",
        "V7EwxU0DdEhj",
        "8uPcfpU8Ko15"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNM+MXyzDwcz098puEWZPjy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "804c26f55ec24cb8b4f11f0a29af6f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e6f179543d544e3906c9ecc8c4b4786",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6d65e9fb3bb4bd799e9daac8936e00c",
              "IPY_MODEL_8482fbb731b644379e94393601dc752f"
            ]
          }
        },
        "3e6f179543d544e3906c9ecc8c4b4786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6d65e9fb3bb4bd799e9daac8936e00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b9ad2e6572a40ed8de5fbeb052e9bde",
            "_dom_classes": [],
            "description": "Dl Completed...:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ed3485f6b4b4e988a5159489b9ea1f0"
          }
        },
        "8482fbb731b644379e94393601dc752f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4f209d470a024fbbbcf3e08335b4a1c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [00:08&lt;?, ? url/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fda6524fde2415ca23ac7bf94b53b6e"
          }
        },
        "1b9ad2e6572a40ed8de5fbeb052e9bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ed3485f6b4b4e988a5159489b9ea1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f209d470a024fbbbcf3e08335b4a1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fda6524fde2415ca23ac7bf94b53b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab9f3f120866427488e87135faab8b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a470856269e3430fba71af49118435a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a09241aa5cb46f9af7bc34c018e7fcf",
              "IPY_MODEL_f7fdda82d7dc4c4a9f5e5281e4ab27bf"
            ]
          }
        },
        "a470856269e3430fba71af49118435a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a09241aa5cb46f9af7bc34c018e7fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6caef56e1aa44499b3afee219219508",
            "_dom_classes": [],
            "description": "Dl Size...:  48%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d39b64506b6446d399b8cfa6916e7f90"
          }
        },
        "f7fdda82d7dc4c4a9f5e5281e4ab27bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7754b5b70944e5c8b84a929b6407e2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 38/80 [00:08&lt;00:09,  4.48 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1b087ebdfe94edd9833a8f6043ae64e"
          }
        },
        "c6caef56e1aa44499b3afee219219508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d39b64506b6446d399b8cfa6916e7f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7754b5b70944e5c8b84a929b6407e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1b087ebdfe94edd9833a8f6043ae64e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-lu/MovieSentiment/blob/main/RL_for_Movie_Review_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJ_liSIjjFg"
      },
      "source": [
        "# TF RL Model\n",
        "\n",
        "*    Policy Gradient with LSTM/BERT model\n",
        "*    comparison to normal BERT fine tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNRL0cDjS6E"
      },
      "source": [
        "### basic classification fine-tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z-GdbMc6qA6"
      },
      "source": [
        "# from nltk import sent_tokenize as sentence_tokenize\n",
        "# import transformers\n",
        "# from transformers import BertTokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8YaM_FlCRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c52d82-1138-42c3-af6e-dfda0ee030a8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version:  2.3.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.10.0\n",
            "GPU is NOT AVAILABLE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524,
          "referenced_widgets": [
            "804c26f55ec24cb8b4f11f0a29af6f56",
            "3e6f179543d544e3906c9ecc8c4b4786",
            "c6d65e9fb3bb4bd799e9daac8936e00c",
            "8482fbb731b644379e94393601dc752f",
            "1b9ad2e6572a40ed8de5fbeb052e9bde",
            "2ed3485f6b4b4e988a5159489b9ea1f0",
            "4f209d470a024fbbbcf3e08335b4a1c4",
            "0fda6524fde2415ca23ac7bf94b53b6e",
            "ab9f3f120866427488e87135faab8b3f",
            "a470856269e3430fba71af49118435a0",
            "1a09241aa5cb46f9af7bc34c018e7fcf",
            "f7fdda82d7dc4c4a9f5e5281e4ab27bf",
            "c6caef56e1aa44499b3afee219219508",
            "d39b64506b6446d399b8cfa6916e7f90",
            "c7754b5b70944e5c8b84a929b6407e2e",
            "d1b087ebdfe94edd9833a8f6043ae64e"
          ]
        },
        "id": "IcutlDDhj-YA",
        "outputId": "d73968c1-3cf8-438d-9103-c07ecb49a21e"
      },
      "source": [
        "train_data, test_data = tfds.load('imdb_reviews', split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "804c26f55ec24cb8b4f11f0a29af6f56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progreâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab9f3f120866427488e87135faab8b3f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressStyâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-366aae33940d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data, test_data = tfds.load('imdb_reviews', split=[\"train\", \"test\"], \n\u001b[0;32m----> 2\u001b[0;31m                                   batch_size=-1, as_supervised=True)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m         prepare_split_kwargs)\n\u001b[1;32m    961\u001b[0m     for split_generator in self._split_generators(\n\u001b[0;32m--> 962\u001b[0;31m         dl_manager, **split_generators_kwargs):\n\u001b[0m\u001b[1;32m    963\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/text/imdb.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0march_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DOWNLOAD_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;31m# Add progress bar to follow the download state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0miter_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (Promise, Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0masync_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/promise/async_.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# fulfilled or rejected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdrain_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/promise/schedulers/immediate.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpromise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_then\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mwaited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwaited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ndjX-tkc3o",
        "outputId": "9e37666b-7e86-4384-a88a-1b661c986bb5"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
        "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\", output_shape=[20], input_shape=[], dtype=tf.string, trainable=True)\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam',loss=tf.losses.BinaryCrossentropy(from_logits=True),metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 20)                400020    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 400,373\n",
            "Trainable params: 400,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oAuC99UTz6Q"
      },
      "source": [
        "x_val = train_examples[:10000]\n",
        "partial_x_train = train_examples[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2sbZk1TaIbO",
        "outputId": "a27aa5f5-d6cf-483c-f37b-f53e56827ac8"
      },
      "source": [
        "type(x_val)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcm2ugMIT00Q",
        "outputId": "a1a7c8f7-8e97-4728-a97a-2eb01e3989ba"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 2s 58ms/step - loss: 0.7434 - accuracy: 0.5263 - val_loss: 0.6633 - val_accuracy: 0.5984\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.6431 - accuracy: 0.6335 - val_loss: 0.6200 - val_accuracy: 0.6558\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.5964 - accuracy: 0.6854 - val_loss: 0.5812 - val_accuracy: 0.7020\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.5518 - accuracy: 0.7329 - val_loss: 0.5427 - val_accuracy: 0.7397\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.5066 - accuracy: 0.7675 - val_loss: 0.5054 - val_accuracy: 0.7648\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.4641 - accuracy: 0.8007 - val_loss: 0.4697 - val_accuracy: 0.7917\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.4224 - accuracy: 0.8262 - val_loss: 0.4388 - val_accuracy: 0.8044\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.3857 - accuracy: 0.8465 - val_loss: 0.4139 - val_accuracy: 0.8184\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.3529 - accuracy: 0.8626 - val_loss: 0.3909 - val_accuracy: 0.8275\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.3243 - accuracy: 0.8771 - val_loss: 0.3707 - val_accuracy: 0.8400\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.2980 - accuracy: 0.8879 - val_loss: 0.3551 - val_accuracy: 0.8478\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.2757 - accuracy: 0.8979 - val_loss: 0.3427 - val_accuracy: 0.8534\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.2560 - accuracy: 0.9048 - val_loss: 0.3354 - val_accuracy: 0.8575\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.2393 - accuracy: 0.9113 - val_loss: 0.3276 - val_accuracy: 0.8617\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.2225 - accuracy: 0.9203 - val_loss: 0.3186 - val_accuracy: 0.8658\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.2075 - accuracy: 0.9264 - val_loss: 0.3140 - val_accuracy: 0.8685\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.1945 - accuracy: 0.9307 - val_loss: 0.3102 - val_accuracy: 0.8704\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.1828 - accuracy: 0.9369 - val_loss: 0.3125 - val_accuracy: 0.8715\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1715 - accuracy: 0.9411 - val_loss: 0.3064 - val_accuracy: 0.8736\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1611 - accuracy: 0.9448 - val_loss: 0.3062 - val_accuracy: 0.8738\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1510 - accuracy: 0.9511 - val_loss: 0.3061 - val_accuracy: 0.8756\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1422 - accuracy: 0.9555 - val_loss: 0.3100 - val_accuracy: 0.8746\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.1333 - accuracy: 0.9596 - val_loss: 0.3091 - val_accuracy: 0.8752\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1259 - accuracy: 0.9630 - val_loss: 0.3108 - val_accuracy: 0.8756\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.1183 - accuracy: 0.9659 - val_loss: 0.3147 - val_accuracy: 0.8749\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.1111 - accuracy: 0.9683 - val_loss: 0.3160 - val_accuracy: 0.8746\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.1035 - accuracy: 0.9722 - val_loss: 0.3189 - val_accuracy: 0.8751\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0963 - accuracy: 0.9746 - val_loss: 0.3247 - val_accuracy: 0.8746\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0899 - accuracy: 0.9772 - val_loss: 0.3288 - val_accuracy: 0.8748\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.0838 - accuracy: 0.9800 - val_loss: 0.3359 - val_accuracy: 0.8723\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.0787 - accuracy: 0.9815 - val_loss: 0.3402 - val_accuracy: 0.8730\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.0733 - accuracy: 0.9834 - val_loss: 0.3493 - val_accuracy: 0.8715\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.0691 - accuracy: 0.9844 - val_loss: 0.3514 - val_accuracy: 0.8737\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0638 - accuracy: 0.9866 - val_loss: 0.3591 - val_accuracy: 0.8734\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0601 - accuracy: 0.9881 - val_loss: 0.3663 - val_accuracy: 0.8701\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.0558 - accuracy: 0.9898 - val_loss: 0.3716 - val_accuracy: 0.8711\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 2s 50ms/step - loss: 0.0524 - accuracy: 0.9905 - val_loss: 0.3777 - val_accuracy: 0.8711\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0483 - accuracy: 0.9923 - val_loss: 0.3847 - val_accuracy: 0.8702\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0453 - accuracy: 0.9928 - val_loss: 0.3919 - val_accuracy: 0.8706\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0421 - accuracy: 0.9940 - val_loss: 0.3997 - val_accuracy: 0.8704\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4353 - accuracy: 0.8516\n",
            "[0.43531104922294617, 0.851639986038208]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbUbnIMxrlr5"
      },
      "source": [
        "## BERT model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQCcK5Xk_GdM"
      },
      "source": [
        "### imports, get_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQv3v3aNrsj2"
      },
      "source": [
        "!pip3 install --quiet tensorflow\n",
        "!pip3 install --quiet tensorflow_text\n",
        "!pip install -q tf-models-official\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8-UaCAtrsu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f51976-af8f-4b89-9d97-d6621e1bace2"
      },
      "source": [
        "'''\n",
        "imports\n",
        "'''\n",
        "\n",
        "import os, sys, re, shutil\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.python.ops import string_ops\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
        "print('GPU NAME:\\n\\t',tf.config.list_physical_devices('GPU'))\n",
        "'''\n",
        "train_data, test_data = tfds.load('imdb_reviews', split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)\n",
        "'''\n",
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version:  2.3.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.10.0\n",
            "GPU is NOT AVAILABLE\n",
            "GPU NAME:\n",
            "\t []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMe6a9mSrs33"
      },
      "source": [
        "# create the train/validation split\n",
        "def get_datasets(batch=32, seed=42):\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  batch_size = batch\n",
        "  seed = seed\n",
        "\n",
        "  raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/train',\n",
        "      batch_size=batch_size,\n",
        "      validation_split=0.2,\n",
        "      subset='training',\n",
        "      seed=seed)\n",
        "\n",
        "  class_names = raw_train_ds.class_names\n",
        "  train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "  val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/train',\n",
        "      batch_size=batch_size,\n",
        "      validation_split=0.2,\n",
        "      subset='validation',\n",
        "      seed=seed)\n",
        "\n",
        "  val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/test',\n",
        "      batch_size=batch_size)\n",
        "\n",
        "  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return train_ds, val_ds, test_ds\n",
        "\n",
        "# # example\n",
        "#\n",
        "# BS=32\n",
        "# train, validation, test = get_datasets(batch=BS)\n",
        "#"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNkgUcZXuST-"
      },
      "source": [
        "encoder_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "preprocessor_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfcvLt4y_ACa"
      },
      "source": [
        "### BERT model functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzvK3OKlv-75"
      },
      "source": [
        "# tutorial build the model\n",
        "def build_bert_model():\n",
        "  encoder_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "  preprocessor_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'\n",
        "\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(preprocessor_url, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  # encoder same as bert_model\n",
        "  encoder = hub.KerasLayer(encoder_url, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOQyZAbuwUdq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7e366934-21d5-4e1a-acbd-73d76b1a9a46"
      },
      "source": [
        "def compile_bert_model(model,\n",
        "                       training_set,\n",
        "                       loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                       metric=tf.metrics.BinaryAccuracy(),\n",
        "                       lr=3e-5,\n",
        "                       optim='adamw'):\n",
        "  loss = loss\n",
        "  # loss = tf.keras.losses.mean_squared_error(from_logits=True)\n",
        "  metrics = metric\n",
        "  init_lr = lr\n",
        "  epochs = 1\n",
        "\n",
        "\n",
        "  steps_per_epoch = tf.data.experimental.cardinality(training_set).numpy()\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "\n",
        "  # optimizer = optimization.create_optimizer(\n",
        "  #     init_lr=init_lr,\n",
        "  #     num_train_steps=num_train_steps,\n",
        "  #     num_warmup_steps=num_warmup_steps,\n",
        "  #     optimizer_type='adamw'\n",
        "  # )\n",
        "  optim = optimization.create_optimizer(\n",
        "      init_lr=init_lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=int(num_train_steps*.13),\n",
        "      optimizer_type='adamw'\n",
        "  )\n",
        "  model.compile(optimizer=optim,loss=loss,metrics=metrics)\n",
        "\n",
        "'''\n",
        "# example\n",
        "\n",
        "bertmodel = build_bert_model()\n",
        "compile_bert_model(model=bertmodel,training_set=train_ds)\n",
        "\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# example\\n\\nbertmodel = build_bert_model()\\ncompile_bert_model(model=bertmodel,training_set=train_ds)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RDoPVMbZZmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00d3e49-e0fc-4c34-f94c-923597258ad1"
      },
      "source": [
        "BATCH_SIZE=64\n",
        "train_ds, val_ds, test_ds = get_datasets(batch=BATCH_SIZE)\n",
        "cfm = build_bert_model()\n",
        "compile_bert_model(model=cfm,\n",
        "                   training_set=train_ds)\n",
        "\n",
        "text_test = tf.constant(['This movie was great, and I think that everyone should watch it! I am a massive fan of time travel comedies.',\n",
        "             'this was a very bad movie. I have never been a fan of Sherlock Holmes or Watson, but I thought I would give this kooky caper a try. What a mistake!!!'])\n",
        "labels_test = tf.constant([1.0, 0.0])\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method CapturableResourceDeleter.__del__ of <tensorflow.python.training.tracking.tracking.CapturableResourceDeleter object at 0x7f2fe332f160>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py\", line 202, in __del__\n",
            "    self._destroy_resource()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\n",
            "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\n",
            "    *args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n",
            "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\n",
            "    graph_function = self._create_graph_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\n",
            "    capture_by_value=self._capture_by_value),\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\n",
            "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 237, in restored_function_body\n",
            "    return _call_concrete_function(function, inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 74, in _call_concrete_function\n",
            "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 106, in _call_flat\n",
            "    cancellation_manager)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1938, in _call_flat\n",
            "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 579, in call\n",
            "    executor_type=executor_type)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\", line 1192, in partitioned_call\n",
            "    f.add_to_graph(graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 495, in add_to_graph\n",
            "    g._add_function(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3345, in _add_function\n",
            "    gradient)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHRBIhfxLIf"
      },
      "source": [
        "Train with  ```model.fit()``` on full Training Set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59YHYFHCw2tF",
        "outputId": "2344ebb8-8341-4a8a-f3ef-be3d46e5d7ad"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = cfm.fit(x=train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=1)\n",
        "\n",
        "print(history.history)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 136s 433ms/step - loss: 0.4804 - binary_accuracy: 0.7487 - val_loss: 0.3862 - val_binary_accuracy: 0.8144\n",
            "{'loss': [0.48041561245918274], 'binary_accuracy': [0.7487499713897705], 'val_loss': [0.3862094581127167], 'val_binary_accuracy': [0.8144000172615051]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Y-9GTPxOEK"
      },
      "source": [
        "Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opIDCqoWcY4C",
        "outputId": "73ea4969-29b2-4f01-bc64-348ecec582ba"
      },
      "source": [
        "test_loss, test_acc = cfm.evaluate(test_ds)\n",
        "print(\"test loss = {}, test accuracy = {}\".format(test_loss,test_acc))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 67s 171ms/step - loss: 0.3821 - binary_accuracy: 0.8182\n",
            "test loss = 0.3820759057998657, test accuracy = 0.8182399868965149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9I4iZg2poEj",
        "outputId": "b2f0f873-fd47-41a5-a3eb-4eff27ad0f4b"
      },
      "source": [
        "history.history.keys()\n",
        "# print((history.history['binary_accuracy']))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYdhCsqFxPYR",
        "outputId": "56ee75c8-f8b4-4f30-c7ec-8b920b3b3d59"
      },
      "source": [
        "test_loss, test_accuracy = classifier_model.evaluate(test_ds)\n",
        "print(f'Loss: {test_loss}')\n",
        "print(f'Accuracy: {test_accuracy}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 70s 89ms/step - loss: 0.3607 - binary_accuracy: 0.8317\n",
            "Loss: 0.36073803901672363\n",
            "Accuracy: 0.8316799998283386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA5JfLV2jXgW"
      },
      "source": [
        "## LSTM\n",
        "* Training LSTM with RL architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QvM-v-8CZR"
      },
      "source": [
        "### step and train functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6l_Ak6fuS8z"
      },
      "source": [
        "DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
        "'''\n",
        "def step(model, X):\n",
        "  # this function uses the LSTM to delete words from sentences\n",
        "  # \n",
        "  # for each word, the LSTM produces a policy as a bicategoriical distribution object\n",
        "  # use each word's policy to sample an action (0==keep, 1==delete)\n",
        "  # collect the summed log probabilities for each action to use as update weights\n",
        "  # \n",
        "  # return the outputs for each word in each sentence in the batch X\n",
        "  # return the actions for each sentence in the batch X\n",
        "  # return the summed log probabilities for each sentence in the batch X\n",
        "  batch_logits = model(X)\n",
        "  batch_actions =[[] for _ in batch_logits]\n",
        "  log_prob_sums =[None for _ in batch_logits]\n",
        "  new_X = []\n",
        "\n",
        "  for idx,sentence in enumerate(batch_logits):\n",
        "    temp_log_probs = []\n",
        "    start=time.time()\n",
        "    for probs in sentence:\n",
        "      # make a distribution for each word\n",
        "      pol = tfp.distributions.Categorical(probs=tf.nn.softmax(probs))\n",
        "      # sample from the distribution\n",
        "      act = pol.sample()\n",
        "      # save the action (0=keep,1=delete)\n",
        "      batch_actions[idx].append(act)\n",
        "      # with that action, get the log_prob\n",
        "      log_prob = pol.log_prob(act)\n",
        "      # save that log_prob to a list \n",
        "      temp_log_probs.append(log_prob)\n",
        "    # if idx==0: print(\"\\tpolicy, action sampling, and log_prob time : \",time.time()-start)\n",
        "    # sum the log probs for this sentence and add to list\n",
        "    log_prob_sums[idx] = sum(temp_log_probs)\n",
        "\n",
        "    x = str(X[idx].numpy(),'utf-8')\n",
        "    acts = batch_actions[idx]\n",
        "    \n",
        "    tmp_str = string_ops.regex_replace(x,DEFAULT_STRIP_REGEX,'')\n",
        "    tmp_str = str(tmp_str.numpy(), 'utf-8').strip()\n",
        "    tmp_list = [i for i in tmp_str.split(' ') if i]\n",
        "\n",
        "    if (len(tmp_list) > len(acts)):\n",
        "      print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      sys.exit()\n",
        "      # _templist = [i for i in tmp_str.split(' ') if i]\n",
        "      # if len(acts) != len(_templist):\n",
        "      #   print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      #   print('_templist len : ',len(_templist))\n",
        "      #   print(_templist)\n",
        "      #   print('acts len : ', len(acts))\n",
        "      #   sys.exit()\n",
        "      # # print('{} ERROR: more tokens than actions...'.format(idx))\n",
        "    new_list = []\n",
        "    for i,toke in enumerate(tmp_list):\n",
        "      if acts[i] != 1:\n",
        "        new_list.append(toke)\n",
        "    new_X.append(' '.join(new_list))\n",
        "    if idx==0: print(\"\\tsentence action and word-removal time : \",time.time()-start)\n",
        "\n",
        "  return batch_logits, new_X, log_prob_sums\n",
        "'''\n",
        "\n",
        "\n",
        "def step2(model, X):\n",
        "  batch_logits = model(X)\n",
        "  batch_actions =[[] for _ in batch_logits]\n",
        "  log_prob_sums =[0.0 for _ in batch_logits]\n",
        "  _log_prob_sums =[None for _ in batch_logits]\n",
        "  new_X = []\n",
        "  words_removed = []\n",
        "  for idx,sentence in enumerate(batch_logits):\n",
        "    temp_log_probs = []\n",
        "    start=time.time()\n",
        "\n",
        "    pol = tfp.distributions.Bernoulli(logits=sentence)\n",
        "    act = pol.sample()\n",
        "    batch_actions[idx] = [ac for ac in act]\n",
        "    log_probs = [pol.log_prob(ac) for ac in act]\n",
        "    lpkeep = pol.log_prob([0])\n",
        "    lpdel = pol.log_prob([1])\n",
        "\n",
        "    onect=0\n",
        "    zeroct=0\n",
        "    lp_list = []\n",
        "    for i,ac in enumerate(act):\n",
        "      if int(ac) == 1:\n",
        "        lp_list.append(lpdel[i])\n",
        "        onect+=1\n",
        "      else:\n",
        "        lp_list.append(lpkeep[i])\n",
        "    #     zeroct+=1\n",
        "    # print(onect,\"/\",zeroct)\n",
        "    log_prob_sums[idx] = sum(lp_list)\n",
        "    x = str(X[idx].numpy(),'utf-8')\n",
        "    acts = batch_actions[idx]\n",
        "    tmp_str = string_ops.regex_replace(x,DEFAULT_STRIP_REGEX,'')\n",
        "    tmp_str = str(tmp_str.numpy(), 'utf-8').strip()\n",
        "    tmp_list = [i for i in tmp_str.split(' ') if i]\n",
        "    if (len(tmp_list) > len(acts)):\n",
        "      print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      sys.exit()\n",
        "    new_list = []\n",
        "    addct=0\n",
        "    rem=0\n",
        "    for i,toke in enumerate(tmp_list):\n",
        "      if acts[i] != 1:\n",
        "        new_list.append(toke)\n",
        "      else:\n",
        "        rem+=1\n",
        "    words_removed.append(rem)\n",
        "    new_X.append(' '.join(new_list))\n",
        "    # if idx==0: \n",
        "      # print(\"\\tsentence action and word-removal time : \",time.time()-start)\n",
        "      # print(X[0])\n",
        "      # print(new_X[0])\n",
        "\n",
        "  return batch_logits, new_X, log_prob_sums, np.mean(words_removed)\n",
        "\n",
        "\n",
        "def remove_words(lstm, X):\n",
        "  batch_logits = lstm(X)\n",
        "  batch_actions =[[] for _ in batch_logits]\n",
        "  new_X = []\n",
        "  stats=[]\n",
        "  for idx,sentence in enumerate(batch_logits):\n",
        "    start=time.time()\n",
        "    temp_stats = [0,len(sentence)]\n",
        "    pol = tfp.distributions.Bernoulli(logits=sentence)\n",
        "    act = pol.sample()\n",
        "    batch_actions[idx] = [ac for ac in act]\n",
        "\n",
        "    x = str(X[idx].numpy(),'utf-8')\n",
        "    acts = batch_actions[idx]\n",
        "    tmp_str = string_ops.regex_replace(x,DEFAULT_STRIP_REGEX,'')\n",
        "    tmp_str = str(tmp_str.numpy(), 'utf-8').strip()\n",
        "    tmp_list = [i for i in tmp_str.split(' ') if i]\n",
        "    if (len(tmp_list) > len(acts)):\n",
        "      print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      sys.exit()\n",
        "    new_list = []\n",
        "    addct=0\n",
        "    for i,toke in enumerate(tmp_list):\n",
        "      if acts[i] != 1:\n",
        "        new_list.append(toke)\n",
        "      else:\n",
        "        temp_stats[0] +=1\n",
        "    stats.append(temp_stats)\n",
        "    new_X.append(' '.join(new_list))\n",
        "  return new_X, stats\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1AjHewEc9id"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "# \n",
        "# policy gradient (REINFORCE) training loop with LSTM model and BERT sentiment classifier\n",
        "# classifier must be compiled!\n",
        "# \n",
        "def train_vpg(lstm, optimizer, classifier, training_set, validation_set, epochs=5):\n",
        "  total_start=time.time()\n",
        "  stepct=0\n",
        "  epoch_avg_words_del=[]\n",
        "  # with tf.device('/device:GPU:0'):\n",
        "  for e in range(epochs):\n",
        "    epoch_losses=[]\n",
        "    counter=0\n",
        "    trainlen = len(training_set)\n",
        "    avg_words_removed = []\n",
        "    total_batches=len(training_set)\n",
        "    for idx,ex_lab in enumerate(training_set):\n",
        "    # for idx,ex_lab in enumerate(training_set.take(100)):\n",
        "      starttime=time.time()\n",
        "      example, label = ex_lab[0], ex_lab[1]\n",
        "      counter+=1\n",
        "      print('epoch={}\\tbatch={}/{}'.format(e,counter,total_batches))\n",
        "      stepct+=1\n",
        "      # run through the lstm\n",
        "      logits, new_example, logprob_sums = None, None, None\n",
        "      with tf.GradientTape() as t:\n",
        "        start = time.time()\n",
        "        # logits, new_example, logprob_sums = step(lstm, example)\n",
        "        logits, new_example, logprob_sums, deleted = step2(lstm, example)\n",
        "        avg_words_removed.append(deleted)\n",
        "        if (idx%25==0):\n",
        "          print(\"batch={} words deleted : {}\".format(idx,deleted))\n",
        "        # logging for curiosity\n",
        "        if e==0 and idx==0:\n",
        "          print('step time for one batch: {:.4f}'.format(time.time()-start))\n",
        "\n",
        "        # get BERT evaluations on these\n",
        "        start=time.time()\n",
        "        new_outs = classifier(tf.constant(new_example))\n",
        "        # get bert evaluation on full sentences\n",
        "        baseline_outs = classifier(example)\n",
        "\n",
        "        if e==0 and idx==0:\n",
        "          print('BERT processing time for one batch : {:.4f}'.format(time.time()-start))\n",
        "\n",
        "        # reward LSTM with the improvement over the original sentences\n",
        "        baseline_loss = tf.keras.losses.mean_squared_error(baseline_outs,tf.cast(label,float))\n",
        "        loss_object = tf.keras.losses.mean_squared_error(new_outs,tf.cast(label,float))\n",
        "        # loss_object = tf.keras.losses.binary_crossentropy(new_outs,tf.cast(label,float))\n",
        "        adjusted_loss = loss_object-baseline_loss\n",
        "        # print(\"loss_x' - loss_base : \",np.array(adjusted_loss))\n",
        "        epoch_losses.append(adjusted_loss.numpy())\n",
        "\n",
        "        # use BERT's loss for update:\n",
        "        # use the negative losses to weight the log_probs\n",
        "        weighted_logprobs = -(adjusted_loss)*logprob_sums\n",
        "        # end with tf.GradientTape()\n",
        "\n",
        "      # train bert by fitting on new_examples\n",
        "      # this way, BERT and the LSTM train together\n",
        "      with tf.device('/device:GPU:0'):\n",
        "        history = classifier.fit(x=tf.constant(new_example),\n",
        "                                  y=label,\n",
        "                                  # validation_split=.2,\n",
        "                                  batch_size=len(new_example),\n",
        "                                  epochs=2,\n",
        "                                  verbose=(idx%25==0))\n",
        "      # print(\"BERT training accuracy : \", np.mean(history.history['binary_accuracy']))\n",
        "\n",
        "      # get gradients and apply update\n",
        "      gradients = t.gradient(weighted_logprobs, lstm.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients,lstm.trainable_variables))\n",
        "\n",
        "      epoch_avg_words_del.append(np.mean(avg_words_removed))\n",
        "\n",
        "      if idx == 1: print('batch time {:.2f}'.format(time.time()-starttime))\n",
        "      # end for\n",
        "    print('\\n_____________________________')\n",
        "    print('[{}]\\t avg training loss : {}'.format(e,np.mean(epoch_losses)))\n",
        "    # print('[{}]\\t total train inloss : {}'.format(e,np.sum(epoch_losses)))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      val_loss, val_acc = classifier.evaluate(validation_set)\n",
        "    print(\"[{}] BERT validation loss     :\\t{}\\n\\t\\tvalidation accuracy :\\t{}\".format(idx,val_loss, val_acc))\n",
        "    print('average words removed: ',epoch_avg_words_del)\n",
        "\n",
        "    '''\n",
        "    # validate \n",
        "    validate()\n",
        "    '''\n",
        "\n",
        "    # end for\n",
        "  print('DONE. {:.4f}'.format(time.time()-total_start))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCZ45NO0XrOL"
      },
      "source": [
        "\n",
        "def evaluate_lstm_model(lstm, classifier, testing_set):\n",
        "  test_x=[]\n",
        "  test_y=[]\n",
        "  total=len(testing_set)\n",
        "  stats = []\n",
        "  for idx,example in enumerate(testing_set):\n",
        "    test_y.append(example[1].numpy())\n",
        "    new_x, _stats = remove_words(lstm_model,example[0])\n",
        "    stats.append(_stats) \n",
        "    test_x.append(np.array(new_x))\n",
        "    print('LSTM processing batch',idx,'/',total-1)\n",
        "    if idx==0:\n",
        "      print(\"Sample LSTM sentence output\")\n",
        "      print(example[0][:5])\n",
        "      print(new_x[:5])\n",
        "  test_loss, test_acc = classifier.evaluate(x=test_x,y=test_y)\n",
        "  return test_loss, test_acc, stats\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ6VJNc18lEY"
      },
      "source": [
        "### setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "zTtP1QmMlRtg",
        "outputId": "f1a78902-041a-4a1b-8c9d-b5f90dfad473"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BUFFER_SIZE = 1000 # vocabulary size?\n",
        "BATCH_SIZE = 32\n",
        "seed = 22\n",
        "\n",
        "train_ds, val_ds, test_ds = get_datasets(batch=BATCH_SIZE)\n",
        "# lstm_train_ds, lstm_val_ds, lstm_train_ds = get_datasets()\n",
        "\n",
        "text_test = tf.constant(['This movie was great, and I think that everyone should watch it! I am a massive fan of time travel comedies.',\n",
        "             'this was a very bad movie. I have never been a fan of Sherlock Holmes or Watson, but I thought I would give this kooky caper a try. What a mistake!!!'])\n",
        "labels_test = tf.constant([1.0, 0.0])\n",
        "\n",
        "VOCAB_SIZE=1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_ds.map(lambda text, label: text))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bd0e2fbe953c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# lstm_train_ds, lstm_val_ds, lstm_train_ds = get_datasets()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_datasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax1VDY1H67yw"
      },
      "source": [
        "# instantiate the BERT model and classify on two sentences\n",
        "classifier_model = build_bert_model()\n",
        "compile_bert_model(model=classifier_model,\n",
        "                   training_set=train_ds)\n",
        "# with tf.device('/device:GPU:0'):\n",
        "#   history = classifier_model.fit(x=text_test, y=labels_test, batch_size=1, epochs=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fZpTYcQ638M"
      },
      "source": [
        "# instantiate LSTM \n",
        "lstm_model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "lstm_optim = tf.optimizers.Adam()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3cDUrEi8GCe"
      },
      "source": [
        "### train_lstm_vpg "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hnQ-8izxDDG"
      },
      "source": [
        "train LSTM on dataset \n",
        "* tune BERT, then train LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPVDEdJXAKZv",
        "outputId": "b10ec6be-3600-4548-efe6-dddad7545d23"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = classifier_model.fit(x=train_ds,\n",
        "                                validation_data=val_ds.shard(10,0),\n",
        "                                epochs=1)\n",
        "  print(history.history)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 127s 203ms/step - loss: 0.4441 - binary_accuracy: 0.7764 - val_loss: 0.3782 - val_binary_accuracy: 0.8223\n",
            "{'loss': [0.4441312849521637], 'binary_accuracy': [0.7763500213623047], 'val_loss': [0.37818530201911926], 'val_binary_accuracy': [0.822265625]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHKYKrd9QqCl"
      },
      "source": [
        "example of accuracy post-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXGC8ewRDMlG",
        "outputId": "fa7dc383-05aa-4fd8-cd2c-04314709db16"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  loss, acc = classifier_model.evaluate(test_ds.shard(30,0))\n",
        "print(loss, acc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27/27 [==============================] - 2s 79ms/step - loss: 0.3794 - binary_accuracy: 0.8113\n",
            "0.3794430196285248 0.8113425970077515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_bItnwQtmI"
      },
      "source": [
        "train LSTM with VPG\n",
        "* reward to LSTM = `-(loss_short - loss_original)`\n",
        "* weighted by `sum(action_log_probs)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rDWXUsqo85jY",
        "outputId": "d4a8aa50-f3fa-4b5b-8a21-65b092b6aff0"
      },
      "source": [
        "train_vpg(lstm_model, lstm_optim, classifier_model, train_ds, val_ds.shard(10,2), epochs=1)\n",
        "print('\\n_______________________________________________________')\n",
        "test_loss, test_accuracy, stats = evaluate_lstm_model(lstm_model, classifier_model, test_ds.shard(690,1))\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "means=[]\n",
        "for batch in stats:\n",
        "  batch_means=[]\n",
        "  for pair in batch:\n",
        "    batch_means.append(pair[0])\n",
        "  means.append(np.mean(batch_means))\n",
        "print(\"mean deletions: \",np.mean(means))\n",
        "print('\\n_______________________________________________________')\n",
        "test_loss, test_accuracy = classifier_model.evaluate(test_ds)\n",
        "print(f'(no LSTM) Test Loss: {test_loss}')\n",
        "print(f'(no LSTM)Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# \n",
        "# mlp experiment\n",
        "# \n",
        "mlp_model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        mask_zero=False),\n",
        "    tf.keras.layers.Dense(64, activation='tanh'),\n",
        "    tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "mlp_optim = tf.optimizers.Adam()\n",
        "\n",
        "train_vpg_mlp(mlp_model,\n",
        "              optimizer=mlp_optim,\n",
        "              classifier=classifier_model2,\n",
        "              training_set=train_ds,\n",
        "              validation_set=val_ds,\n",
        "              epochs=1)\n",
        "loss, acc, stat = evaluate_mlp_model(mlp=mlp_model, classifier=mlp_classifier)\n",
        "means=[]\n",
        "for batch in stat:\n",
        "  batch_means=[]\n",
        "  for pair in batch:\n",
        "    batch_means.append(pair[0])\n",
        "  means.append(np.mean(batch_means))\n",
        "print('avg words removed: ',np.mean(means))\n",
        "# print(\"avg removed words: \", np.mean(stat[:][0]))\n",
        "print(\"eval acc: \", acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=0\tbatch=1/625\n",
            "batch=0 words deleted : 93.40625\n",
            "step time for one batch: 21.2424\n",
            "BERT processing time for one batch : 0.2190\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.4998 - binary_accuracy: 0.7600 - val_loss: 0.7819 - val_binary_accuracy: 0.5714\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5230 - binary_accuracy: 0.7600 - val_loss: 0.7819 - val_binary_accuracy: 0.5714\n",
            "batch time 28.56\n",
            "epoch=0\tbatch=26/625\n",
            "batch=25 words deleted : 142.65625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.5124 - binary_accuracy: 0.7200 - val_loss: 0.1934 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4529 - binary_accuracy: 0.7200 - val_loss: 0.1934 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=51/625\n",
            "batch=50 words deleted : 68.40625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3689 - binary_accuracy: 0.8000 - val_loss: 0.4258 - val_binary_accuracy: 0.7143\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3915 - binary_accuracy: 0.8000 - val_loss: 0.4258 - val_binary_accuracy: 0.7143\n",
            "epoch=0\tbatch=76/625\n",
            "batch=75 words deleted : 4.09375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5553 - binary_accuracy: 0.8400 - val_loss: 0.3164 - val_binary_accuracy: 0.7143\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4460 - binary_accuracy: 0.8000 - val_loss: 0.3164 - val_binary_accuracy: 0.7143\n",
            "epoch=0\tbatch=101/625\n",
            "batch=100 words deleted : 1.96875\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3847 - binary_accuracy: 0.8400 - val_loss: 0.1882 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4209 - binary_accuracy: 0.8000 - val_loss: 0.1882 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=126/625\n",
            "batch=125 words deleted : 0.9375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.2719 - binary_accuracy: 0.8400 - val_loss: 0.3327 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.2509 - binary_accuracy: 0.8800 - val_loss: 0.3327 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=151/625\n",
            "batch=150 words deleted : 0.5\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3011 - binary_accuracy: 0.8000 - val_loss: 0.1406 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.2898 - binary_accuracy: 0.8000 - val_loss: 0.1406 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=176/625\n",
            "batch=175 words deleted : 0.25\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3434 - binary_accuracy: 0.8400 - val_loss: 0.1382 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3104 - binary_accuracy: 0.8000 - val_loss: 0.1382 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=201/625\n",
            "batch=200 words deleted : 0.03125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.4822 - binary_accuracy: 0.7200 - val_loss: 0.2327 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4108 - binary_accuracy: 0.7200 - val_loss: 0.2327 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=226/625\n",
            "batch=225 words deleted : 0.03125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.2582 - binary_accuracy: 0.9600 - val_loss: 0.1855 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.2937 - binary_accuracy: 0.9200 - val_loss: 0.1855 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=251/625\n",
            "batch=250 words deleted : 0.09375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.4700 - binary_accuracy: 0.7200 - val_loss: 0.1476 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3842 - binary_accuracy: 0.8000 - val_loss: 0.1476 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=276/625\n",
            "batch=275 words deleted : 0.03125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3431 - binary_accuracy: 0.8800 - val_loss: 0.1813 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3075 - binary_accuracy: 0.8800 - val_loss: 0.1813 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=301/625\n",
            "batch=300 words deleted : 0.09375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.2999 - binary_accuracy: 0.8800 - val_loss: 0.2527 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.2686 - binary_accuracy: 0.8400 - val_loss: 0.2527 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=326/625\n",
            "batch=325 words deleted : 0.15625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.1520 - binary_accuracy: 0.9200 - val_loss: 0.1516 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1757 - binary_accuracy: 0.9200 - val_loss: 0.1516 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=351/625\n",
            "batch=350 words deleted : 0.15625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3811 - binary_accuracy: 0.7600 - val_loss: 0.3017 - val_binary_accuracy: 0.7143\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3496 - binary_accuracy: 0.7600 - val_loss: 0.3017 - val_binary_accuracy: 0.7143\n",
            "epoch=0\tbatch=376/625\n",
            "batch=375 words deleted : 0.25\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.2959 - binary_accuracy: 0.9200 - val_loss: 0.7665 - val_binary_accuracy: 0.5714\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.2766 - binary_accuracy: 0.9200 - val_loss: 0.7665 - val_binary_accuracy: 0.5714\n",
            "epoch=0\tbatch=401/625\n",
            "batch=400 words deleted : 0.3125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.1915 - binary_accuracy: 0.9200 - val_loss: 0.1390 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.2015 - binary_accuracy: 0.9200 - val_loss: 0.1390 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=426/625\n",
            "batch=425 words deleted : 0.40625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3200 - binary_accuracy: 0.8400 - val_loss: 0.0720 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3576 - binary_accuracy: 0.8800 - val_loss: 0.0720 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=451/625\n",
            "batch=450 words deleted : 0.125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3914 - binary_accuracy: 0.7200 - val_loss: 0.1055 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4019 - binary_accuracy: 0.7200 - val_loss: 0.1055 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=476/625\n",
            "batch=475 words deleted : 0.34375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3676 - binary_accuracy: 0.8400 - val_loss: 0.3121 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.4073 - binary_accuracy: 0.8000 - val_loss: 0.3121 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=501/625\n",
            "batch=500 words deleted : 0.375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.4382 - binary_accuracy: 0.7200 - val_loss: 0.8343 - val_binary_accuracy: 0.7143\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4311 - binary_accuracy: 0.7600 - val_loss: 0.8343 - val_binary_accuracy: 0.7143\n",
            "epoch=0\tbatch=526/625\n",
            "batch=525 words deleted : 0.1875\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.4701 - binary_accuracy: 0.7200 - val_loss: 0.2966 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.4685 - binary_accuracy: 0.7200 - val_loss: 0.2966 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=551/625\n",
            "batch=550 words deleted : 0.03125\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3405 - binary_accuracy: 0.7200 - val_loss: 0.1641 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3850 - binary_accuracy: 0.7200 - val_loss: 0.1641 - val_binary_accuracy: 1.0000\n",
            "epoch=0\tbatch=576/625\n",
            "batch=575 words deleted : 0.15625\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.6320 - binary_accuracy: 0.7200 - val_loss: 0.6593 - val_binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.6335 - binary_accuracy: 0.7600 - val_loss: 0.6593 - val_binary_accuracy: 0.8571\n",
            "epoch=0\tbatch=601/625\n",
            "batch=600 words deleted : 0.09375\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.4211 - binary_accuracy: 0.8400 - val_loss: 0.0854 - val_binary_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3821 - binary_accuracy: 0.8400 - val_loss: 0.0854 - val_binary_accuracy: 1.0000\n",
            "\n",
            "_____________________________\n",
            "[0]\t avg training loss : -0.021967751905322075\n",
            "16/16 [==============================] - 1s 76ms/step - loss: 0.3646 - binary_accuracy: 0.8418\n",
            "[624] BERT validation loss     :\t0.3645973801612854\n",
            "\t\tvalidation accuracy :\t0.841796875\n",
            "average words removed:  [93.40625, 100.015625, 104.39583333333333, 113.5078125, 112.975, 116.75, 116.84821428571429, 119.171875, 117.05208333333333, 117.50625, 118.41477272727273, 117.19010416666667, 115.67067307692308, 117.38839285714286, 117.56875, 117.017578125, 115.86029411764706, 116.30381944444444, 116.01644736842105, 115.75, 115.58482142857143, 115.89488636363636, 114.90896739130434, 115.01953125, 115.67625, 116.7139423076923, 116.40046296296296, 116.0, 115.45043103448276, 114.321875, 113.53629032258064, 113.0341796875, 113.40625, 112.875, 112.36875, 112.16059027777777, 111.9045608108108, 111.93503289473684, 112.46875, 112.29765625, 112.14634146341463, 111.99107142857143, 111.66351744186046, 112.27911931818181, 111.97083333333333, 112.24796195652173, 111.99468085106383, 111.40625, 111.07461734693878, 110.68875, 109.85968137254902, 109.1484375, 108.51768867924528, 108.02256944444444, 107.5409090909091, 107.40792410714286, 106.61293859649123, 106.25808189655173, 105.80508474576271, 104.76197916666666, 103.44108606557377, 101.95816532258064, 100.46329365079364, 98.9873046875, 97.52788461538462, 96.1122159090909, 94.72527985074628, 93.37821691176471, 92.06476449275362, 90.80491071428571, 89.56382042253522, 88.36024305555556, 87.1896404109589, 86.0460304054054, 84.93625, 83.87253289473684, 82.82386363636364, 81.79727564102564, 80.80340189873418, 79.825, 78.87422839506173, 77.9435975609756, 77.03539156626506, 76.14471726190476, 75.27757352941177, 74.43495639534883, 73.60668103448276, 72.79651988636364, 72.00807584269663, 71.23333333333333, 70.47630494505495, 69.73233695652173, 69.00739247311827, 68.30053191489361, 67.60065789473684, 66.91796875, 66.2458118556701, 65.59151785714286, 64.94823232323232, 64.3246875, 63.70730198019802, 63.10079656862745, 62.51092233009709, 61.93179086538461, 61.364583333333336, 60.803950471698116, 60.25730140186916, 59.71875, 59.19466743119266, 58.67642045454546, 58.164414414414416, 57.658482142857146, 57.162887168141594, 56.67790570175438, 56.20326086956522, 55.7354525862069, 55.27564102564103, 54.82150423728814, 54.37867647058823, 53.93984375, 53.508780991735534, 53.08504098360656, 52.666158536585364, 52.250756048387096, 51.84075, 51.43675595238095, 51.04084645669291, 50.64794921875, 50.26187015503876, 49.88149038461538, 49.505486641221374, 49.13494318181818, 48.77161654135338, 48.41044776119403, 48.05393518518518, 47.70450367647059, 47.36108576642336, 47.02151268115942, 46.68547661870504, 46.355357142857144, 46.02969858156028, 45.709286971830984, 45.39182692307692, 45.07964409722222, 44.77090517241379, 44.46618150684932, 44.16751700680272, 43.872255067567565, 43.58221476510067, 43.295625, 43.01221026490066, 42.73396381578947, 42.45710784313726, 42.18486201298701, 41.91633064516129, 41.65124198717949, 41.388734076433124, 41.13073575949367, 40.874410377358494, 40.621484375, 40.3728649068323, 40.126543209876544, 39.88343558282209, 39.644245426829265, 39.40625, 39.17112198795181, 38.9375, 38.70796130952381, 38.48150887573964, 38.25698529411765, 38.03581871345029, 37.817405523255815, 37.601336705202314, 37.38703304597701, 37.175357142857145, 36.96555397727273, 36.75882768361582, 36.55372191011236, 36.35090782122905, 36.15052083333333, 35.95269337016575, 35.75600961538461, 35.56232923497268, 35.370414402173914, 35.180236486486486, 34.992103494623656, 34.806985294117645, 34.622672872340424, 34.44064153439153, 34.26036184210526, 34.08180628272251, 33.905924479166664, 33.73089378238342, 33.55782860824742, 33.386057692307695, 33.216517857142854, 33.048857868020306, 32.88289141414141, 32.718435929648244, 32.5553125, 32.393501243781095, 32.23375618811881, 32.07543103448276, 31.919117647058822, 31.763719512195124, 31.60983009708738, 31.457125603864736, 31.30603966346154, 31.156848086124402, 31.008928571428573, 30.862707345971565, 30.717423349056602, 30.573356807511736, 30.430490654205606, 30.28938953488372, 30.14959490740741, 30.01094470046083, 29.873566513761467, 29.737585616438356, 29.602698863636363, 29.46889140271493, 29.33643018018018, 29.205297085201792, 29.075334821428573, 28.946527777777778, 28.81858407079646, 28.69218061674009, 28.56688596491228, 28.442276200873362, 28.318885869565218, 28.196834415584416, 28.075835129310345, 27.95574034334764, 27.837606837606838, 27.719148936170214, 27.602092161016948, 27.48589135021097, 27.370535714285715, 27.256276150627617, 27.142708333333335, 27.03060165975104, 26.919421487603305, 26.80877057613169, 26.699154713114755, 26.590561224489797, 26.4828506097561, 26.37651821862348, 26.271043346774192, 26.165537148594378, 26.06175, 25.95829183266932, 25.855654761904763, 25.7538290513834, 25.652805118110237, 25.552450980392155, 25.4527587890625, 25.3539640077821, 25.25593507751938, 25.158663127413128, 25.0625, 24.96683429118774, 24.87213740458015, 24.77768536121673, 24.684185606060606, 24.59139150943396, 24.499295112781954, 24.407771535580526, 24.317047574626866, 24.226881970260223, 24.13773148148148, 24.048777675276753, 23.960822610294116, 23.873511904761905, 23.786610401459853, 23.700454545454544, 23.614696557971016, 23.529783393501805, 23.445481115107913, 23.361783154121863, 23.27857142857143, 23.196063167259787, 23.11402925531915, 23.03268551236749, 22.951804577464788, 22.872149122807016, 22.792395104895103, 22.713087979094077, 22.63454861111111, 22.556877162629757, 22.479202586206895, 22.402384020618555, 22.326091609589042, 22.250319965870307, 22.17517006802721, 22.10042372881356, 22.02607685810811, 21.95223063973064, 21.878670302013422, 21.806229096989966, 21.733854166666667, 21.661960132890364, 21.590438741721854, 21.519595709570957, 21.44911595394737, 21.378995901639342, 21.309334150326798, 21.24012622149837, 21.171570616883116, 21.103357605177994, 21.03548387096774, 20.9679461414791, 20.900941506410255, 20.83446485623003, 20.76841162420382, 20.70277777777778, 20.637559335443036, 20.5727523659306, 20.50815644654088, 20.444063479623825, 20.3802734375, 20.31697819314642, 20.254270186335404, 20.191660216718265, 20.129533179012345, 20.067692307692308, 20.00661426380368, 19.945814220183486, 19.8851943597561, 19.825037993920972, 19.765435606060606, 19.706004531722055, 19.647025602409638, 19.588213213213212, 19.530220808383234, 19.4722947761194, 19.414527529761905, 19.357566765578635, 19.300758136094675, 19.24428466076696, 19.188051470588235, 19.131964809384165, 19.076206140350877, 19.02141034985423, 18.967114825581394, 18.91268115942029, 18.858742774566473, 18.805115273775215, 18.75134698275862, 18.698513610315185, 18.645803571428573, 18.59312678062678, 18.540571732954547, 18.48849150141643, 18.436970338983052, 18.38538732394366, 18.334269662921347, 18.28361344537815, 18.233152932960895, 18.182625348189415, 18.132986111111112, 18.08336218836565, 18.033839779005525, 17.984676308539946, 17.935868818681318, 17.887414383561644, 17.839224726775956, 17.791382833787466, 17.743716032608695, 17.696476964769648, 17.649577702702704, 17.60335242587601, 17.556871639784948, 17.510304959785522, 17.46398729946524, 17.418583333333334, 17.37292220744681, 17.327669098143236, 17.28249007936508, 17.237796833773086, 17.193667763157894, 17.1492782152231, 17.105693717277486, 17.063071148825067, 17.019856770833332, 16.976704545454545, 16.933613989637305, 16.891149870801033, 16.848340850515463, 16.805671593830333, 16.763141025641026, 16.72050831202046, 16.67841198979592, 16.63684796437659, 16.59517766497462, 16.55403481012658, 16.51270517676768, 16.471977329974813, 16.431375628140703, 16.390664160401002, 16.350390625, 16.310395885286784, 16.27021144278607, 16.230303970223325, 16.190594059405942, 16.151466049382716, 16.11222290640394, 16.07301904176904, 16.034466911764707, 15.996026894865526, 15.957621951219512, 15.919555961070559, 15.881447208737864, 15.843901331719128, 15.806385869565217, 15.76867469879518, 15.731670673076923, 15.694619304556355, 15.657819976076555, 15.62119630071599, 15.585119047619047, 15.549361638954869, 15.513329383886257, 15.47724586288416, 15.441774764150944, 15.40625, 15.371038732394366, 15.335992388758783, 15.301255841121495, 15.26602564102564, 15.231468023255815, 15.196925754060326, 15.16261574074074, 15.128464203233257, 15.09447004608295, 15.060919540229886, 15.026877866972477, 14.992991990846681, 14.959760273972602, 14.92611047835991, 14.892755681818182, 14.859197845804989, 14.826216063348417, 14.793030474040632, 14.75999436936937, 14.727457865168539, 14.694927130044842, 14.662681767337808, 14.630022321428571, 14.59771714922049, 14.565555555555555, 14.533536585365853, 14.501797566371682, 14.470129690949227, 14.438670154185022, 14.407348901098901, 14.37609649122807, 14.344912472647703, 14.314001091703057, 14.283088235294118, 14.252445652173913, 14.22193600867679, 14.191761363636363, 14.16137958963283, 14.131196120689655, 14.101209677419355, 14.071150751072961, 14.04142130620985, 14.011551816239317, 13.981676439232409, 13.95219414893617, 13.922837048832271, 13.893736758474576, 13.864891649048626, 13.835772679324894, 13.806907894736842, 13.778623949579831, 13.749934486373165, 13.721495815899582, 13.69304540709812, 13.664973958333333, 13.636759355509355, 13.608791493775934, 13.580874741200828, 13.553137913223141, 13.525515463917525, 13.498199588477366, 13.470931724845997, 13.443711577868852, 13.416538854805726, 13.389285714285714, 13.36239816700611, 13.336064532520325, 13.309457403651116, 13.28302125506073, 13.256565656565657, 13.230342741935484, 13.204225352112676, 13.178150100401606, 13.151928857715431, 13.125875, 13.100424151696608, 13.074950199203187, 13.04932902584493, 13.023685515873016, 12.998205445544555, 12.972702569169961, 12.947731755424064, 12.92273622047244, 12.897593320235757, 12.872732843137255, 12.847541585127201, 12.8228759765625, 12.798123781676413, 12.773650291828794, 12.749150485436893, 12.724624515503876, 12.700253868471954, 12.675977316602317, 12.651794315992293, 12.627704326923077, 12.603706813819578, 12.579621647509578, 12.555688336520076, 12.532084923664122, 12.50845238095238, 12.485028517110266, 12.46157495256167, 12.438151041666666, 12.414874763705104, 12.391686320754717, 12.368467514124294, 12.345277255639099, 12.322115384615385, 12.299215823970037, 12.276343457943925, 12.253439832089553, 12.230737895716945, 12.20812035315985, 12.185760667903525, 12.163310185185185, 12.1409426987061, 12.11865774907749, 12.096512430939226, 12.074333639705882, 12.052350917431193, 12.030448717948717, 12.008626599634368, 11.986713047445255, 11.964993169398907, 11.94340909090909, 11.921789927404719, 11.900249094202898, 11.878955696202532, 11.857569945848375, 11.836261261261262, 11.81531025179856, 11.794210053859963, 11.773185483870968, 11.752180232558139, 11.73125, 11.710394385026738, 11.68983540925267, 11.669127442273535, 11.648492907801419, 11.627931415929204, 11.60755300353357, 11.58724647266314, 11.56684639084507, 11.546682776801406, 11.526480263157895, 11.506403239929947, 11.48628715034965, 11.466513961605585, 11.446700783972126, 11.426902173913044, 11.407335069444445, 11.387619150779896, 11.368133650519031, 11.348607512953368, 11.329202586206897, 11.309864457831326, 11.29053908934708, 11.271440823327616, 11.252193921232877, 11.23301282051282, 11.213950511945393, 11.195006388415672, 11.17607355442177, 11.157311120543294, 11.138453389830508, 11.119712351945854, 11.101140202702704, 11.08241989881956, 11.063867845117844, 11.045483193277311, 11.027055369127517, 11.00858458961474, 10.99033235785953, 10.972141068447412, 10.95390625, 10.935836106489184, 10.91782599667774, 10.899720149253731, 10.881674254966887, 10.86379132231405, 10.846070544554456, 10.828202224052719, 10.810444078947368, 10.792795566502463, 10.775204918032786, 10.757671849427169, 10.740349264705882, 10.722981239804241, 10.705517100977199, 10.68816056910569, 10.670809659090908, 10.653616288492707, 10.636579692556634, 10.619547657512117, 10.60257056451613, 10.585547504025765, 10.568629421221864, 10.5518158105939, 10.535006009615385, 10.5182]\n",
            "DONE. 17475.2444\n",
            "\n",
            "_______________________________________________________\n",
            "LSTM processing batch 0 / 1\n",
            "Sample LSTM sentence output\n",
            "tf.Tensor(\n",
            "[b\"OK,so this film is NOT very well known,and wasn't very well publicised.I discovered this fairly brutal gangster gone good movie by complete accident on one of Skys millions of movie channels late on some boring evening,but I'm glad i did!The opening sequence to this film is fantastically comical in a very dark way.This in fact sets what i think is the general tone for the movie.I think a lot of critics and movie fans that have actually seen this film have been a bit unfair to just write it off as a lower budget gangster movie in the Reservoir Dogs vein.OK,so there are undeniable similarities between Thursday and some other crime genre films that it has been compared to,but in all fairness,i think this film takes a much more darkly comic look at this type of film,and the end result is a engrossing,well made,funny,if not totally original film.Tom Jane is good in this,and deserves the recognition he will now hopefully get thanks to the The Punisher.His performance as the bad guy gone good is realistic,funny and just cold enough to make you believe Casey really was a bad ass before he reformed.Thats another thing that makes this film stand out for me,the characters.In Nicks gang you get the strangest trio of criminals ever assembled,a smooth,charismatic but very cold leader(Nick),a trigger happy blood loving sexually predatory bitch of a woman(Dallas)and a psychotic hill billy with brains with a penchant for torture(Billy Hill).Throw in the most bizarre police detective ever seen on screen,beautifully over played by Mickey Rourke,and you've got a recipe for...well for Thursday really.Its at times darkly comic,sometimes brutal,sometimes unoriginal,but always engrossing and worth watching.8/10\"\n",
            " b'This \"movie\" will give me nightmares, I will wake up drenched in sweat, screaming \"I didn\\'t make this film please don\\'t blame me!\" I honestly think it would have been more entertaining to watch a fat guy eating lard in his moms basement for a hour or two, than to watch this crap. I understand money was tight but goddamn what the hell were they thinking there was no thought, plot or effort put into this. This movie needs a warning \"Please for the love of god don\\'t fund the drama department a the local JC.\" On an other note these are the least likable characters I have ever seen, and I have seen movies with Hitler in them. So lastly take my advice the next time you even think about renting this just pop a few hundred Adivl and let the sleep come.'\n",
            " b\"Horses on Mars is a wonderful journey taken by very small creatures. It doesn't have to work hard to hold your attention and the characters are truly memorable. Most of the credit must go to Eric Anderson for his choice of visual style and for writing an interesting story, but credit is also deserved by Anuj Majumdar for the narration and the wonderful music score. These choices were obviously not an accident. If you can, see it in 35mm scope for the most impact. It is quite a beautiful film!\"\n",
            " b'River\\'s Edge is an extremely disturbing film written by acclaimed American screen writer Neal Jimenez.It is based on an actual event which happened at a time when most of American youngsters were trying to make sense of their lives.This is one of the most outstanding films made by American director Tim Hunter.Much of film\\'s attention is focused on a reckless murder committed by a feckless teenager.This unfortunate event sets in motion a whole range of questions about real motivations of youngsters in American society.Those who saw this film during its initial release must have had vivid memories of great actor Dennis Hopper in a confused role as a sympathetic social outcast. Matrix star Keanu Reeves also looks good as one of the teenagers before he reached star status.At a time when teen flicks are made without any kind of serious preparation,it is hoped that \"River\\'s Edge\" cannot simply be ignored as just another silly teen flick.It had massive impact on people who lived during turbulent times of the past when being an inhabitant of a sleepy town was akin to not having being born.For today\\'s generation with their heady overdoses of Internet props such as Facebook,Twitter and Orkut,River\\'s Edge might appear to be outdated but its importance cannot be denied by any serious film admirer.'\n",
            " b\"Okay. To enjoy this silent comedy short you MUST suspend disbelief concerning the major starting point for the film. If you can't then you'll probably be more likely to score this film a lot lower. Charlie Chase has a HUGE overbite and his wife has a nose large enough to have its own area code. Unknown to each other, they have both been saving to have surgery to correct these defects. Apparently, plastic and dental surgery was better back in the 1920s because neither seemed to have any need to recuperate from these major surgeries and they looked just dandy right away!! Okay, remember I said to ignore this, right?! Okay, well you also have to then ignore the difficult to believe idea that both could then meet and have no idea the other is their spouse. Okay,...now that you allowed yourself to accept these two silly premises, the film gets really, really good.<br /><br />Charlie makes a pass at her and she makes a pass at him. Both are shocked and thrilled because no one has ever really considered them attractive. So, because of this new vanity they agree to go on a date. But, they both sneak back home--not wanting their spouses to know! Anyway, they meet later and are quite attracted to each other. But what about the poor spouses supposedly at home? Well, they both learn that the other is married and both anticipate their marriages will result in divorce because they really want to be with each other! Late in the film, Charlie figures out that the woman really is his wife and he goes through a very funny sequence where he plays both the boyfriend and the old husband--by changing his clothes and putting in false teeth when he plays the hubby! It really is a laugh riot to see him bouncing in and out of the room as he appears to be fighting with another person! You really have to see it to believe it. However, the wife sees an ad with Charlie's before and after photos and knows what's happening. In the end, they both feel pretty foolish!\"], shape=(5,), dtype=string)\n",
            "['OKso this film is NOT very well knownand wasnt very well publicisedI discovered this fairly brutal gangster gone good movie by complete accident on one of Skys millions of movie channels late on some boring eveningbut Im glad i didThe opening sequence to this film is fantastically comical in a very dark wayThis in fact sets what i think is the general tone for the movieI think a lot of critics and movie fans that have actually seen this film have been a bit unfair to just write it off as a lower budget gangster movie in the Reservoir Dogs veinOKso there are undeniable similarities between Thursday and some other crime genre films that it has been compared tobut in all fairnessi think this film takes a much more darkly comic look at this type of filmand the end result is a engrossingwell madefunnyif not totally original filmTom Jane is good in thisand deserves the recognition he will now hopefully get thanks to the The PunisherHis performance as the bad guy gone good is realisticfunny and just cold enough to make you believe Casey really was a bad ass before he reformedThats another thing that makes this film stand out for methe charactersIn Nicks gang you get the strangest trio of criminals ever assembleda smoothcharismatic but very cold leaderNicka trigger happy blood loving sexually predatory bitch of a womanDallasand a psychotic hill billy with brains with a penchant for tortureBilly HillThrow in the most bizarre police detective ever seen on screenbeautifully over played by Mickey Rourkeand youve got a recipe forwell for Thursday reallyIts at times darkly comicsometimes brutalsometimes unoriginalbut always engrossing and worth watching810', 'This movie will give me nightmares I will wake up drenched in sweat screaming I didnt make this film please dont blame me I honestly think it would have been more entertaining to watch a fat guy eating lard in his moms basement for a hour or two than to watch this crap I understand money was tight but goddamn what the hell were they thinking there was no thought plot or effort put into this This movie needs a warning Please for the love of god dont fund the drama department a the local JC On an other note these are the least likable characters I have ever seen and I have seen movies with Hitler in them So lastly take my advice the next time you even think about renting this just pop a few hundred Adivl and let the sleep come', 'Horses on Mars is a wonderful journey taken by very small creatures It doesnt have to work hard to hold your attention and the characters are truly memorable Most of the credit must go to Eric Anderson for his choice of visual style and for writing an interesting story but credit is also deserved by Anuj Majumdar for the narration and the wonderful music score These choices were obviously not an accident If you can see it in 35mm scope for the most impact It is quite a beautiful film', 'Rivers Edge is an extremely disturbing film written by acclaimed American screen writer Neal JimenezIt is based on an actual event which happened at a time when most of American youngsters were trying to make sense of their livesThis is one of the most outstanding films made by American director Tim HunterMuch of films attention is focused on a reckless murder committed by a feckless teenagerThis unfortunate event sets in motion a whole range of questions about real motivations of youngsters in American societyThose who saw this film during its initial release must have had vivid memories of great actor Dennis Hopper in a confused role as a sympathetic social outcast Matrix star Keanu Reeves also looks good as one of the teenagers before he reached star statusAt a time when teen flicks are made without any kind of serious preparationit is hoped that Rivers Edge cannot simply be ignored as just another silly teen flickIt had massive impact on people who lived during turbulent times of the past when being an inhabitant of a sleepy town was akin to not having being bornFor todays generation with their heady overdoses of Internet props such as FacebookTwitter and OrkutRivers Edge might appear to be outdated but its importance cannot be denied by any serious film admirer', 'Okay To enjoy this silent comedy short you MUST suspend disbelief concerning the major starting point for the film If you cant then youll probably be more likely to score this film a lot lower Charlie Chase has a HUGE overbite and his wife has a nose large enough to have its own area code Unknown to each other they have both been saving to have surgery to correct these defects Apparently plastic and dental surgery was better back in the 1920s because neither seemed to have any need to recuperate from these major surgeries and they looked just dandy right away Okay remember I said to ignore this right Okay well you also have to then ignore the difficult to believe idea that both could then meet and have no idea the other is their spouse Okaynow that you allowed yourself to accept these two silly premises the film gets really really goodbr br Charlie makes a pass at her and she makes a pass at him Both are shocked and thrilled because no one has ever really considered them attractive So because of this new vanity they agree to go on a date But they both sneak back homenot wanting their spouses to know Anyway they meet later and are quite attracted to each other But what about the poor spouses supposedly at home Well they both learn that the other is married and both anticipate their marriages will result in divorce because they really want to be with each other Late in the film Charlie figures out that the woman really is his wife and he goes through a very funny sequence where he plays both the boyfriend and the old husbandby changing his clothes and putting in false teeth when he plays the hubby It really is a laugh riot to see him bouncing in and out of the room as he appears to be fighting with another person You really have to see it to believe it However the wife sees an ad with Charlies before and after photos and knows whats happening In the end they both feel pretty foolish']\n",
            "LSTM processing batch 1 / 1\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4242 - binary_accuracy: 0.7812\n",
            "Test Loss: 0.42419105768203735\n",
            "Test Accuracy: 0.78125\n",
            "mean deletions:  0.0\n",
            "\n",
            "_______________________________________________________\n",
            "782/782 [==============================] - 63s 81ms/step - loss: 0.3646 - binary_accuracy: 0.8311\n",
            "(no LSTM) Test Loss: 0.36462676525115967\n",
            "(no LSTM)Test Accuracy: 0.8310800194740295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4611f5c10343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmlp_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m train_vpg_mlp(mlp_model,\n\u001b[0m\u001b[1;32m     34\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_optim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m               \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_model2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_vpg_mlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIoymOd32ysW"
      },
      "source": [
        "LSTM Removes an average of 10 words per at the end of training\n",
        "- 100+ words removed on average in first several batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNWm_bw3RTvn"
      },
      "source": [
        "### lstm_model2 below used for smaller tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7YVu7vO2UnZ",
        "outputId": "c452d9ab-0de8-4f5b-e23f-ab495fbd97ae"
      },
      "source": [
        "import tensorflow as tf\n",
        "train_ds, val_ds, test_ds = get_datasets()\n",
        "VOCAB_SIZE=1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_ds.map(lambda text, label: text))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu1ofsx9psLL"
      },
      "source": [
        "lstm_model2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "lstm_optim2 = tf.optimizers.Adam()\n",
        "classifier_model2 = build_bert_model()\n",
        "compile_bert_model(model=classifier_model2,\n",
        "                   training_set=train_ds)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uPcfpU8Ko15"
      },
      "source": [
        "# MLP (no full training example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJcpaGlUKn_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37de5d3-4f71-4b6a-fbc7-9e03a6e76107"
      },
      "source": [
        "# alternative to LSTM\n",
        "# every word is an individual training example\n",
        "# \n",
        "# striclty worse than LSTM\n",
        "DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
        "VOCAB_SIZE=1000\n",
        "train_ds, val_ds, test_ds = get_datasets()\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_ds.map(lambda text, label: text))\n",
        "\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        mask_zero=False),\n",
        "    tf.keras.layers.Dense(64, activation='tanh'),\n",
        "    tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "mlp_optim = tf.optimizers.Adam()\n",
        "\n",
        "def mlpstep2(model, X):\n",
        "  minibatch=[]\n",
        "  # for idx,x in enumerate(X):\n",
        "  #   string = str(x.numpy(),'utf-8').strip()\n",
        "  #   string = string_ops.regex_replace(string,DEFAULT_STRIP_REGEX,'')\n",
        "  #   string_input = [s for s in string.split(' ') if s]\n",
        "  #   minibatch.append(model(string_input))\n",
        "  #   # minibatch is split strins\n",
        "  #   print(minibatch[idx])\n",
        "  #   sys.exit()\n",
        "  # raw_out=model(X)\n",
        "  # print(raw_out[0]\n",
        "  # print(raw_out.shape)\n",
        "\n",
        "  batch_logits = model(X)\n",
        "  batch_actions =[[] for _ in batch_logits]\n",
        "  log_prob_sums =[0.0 for _ in batch_logits]\n",
        "  _log_prob_sums =[None for _ in batch_logits]\n",
        "  new_X = []\n",
        "  words_removed=[]\n",
        "  for idx,sentence in enumerate(batch_logits):\n",
        "    temp_log_probs = []\n",
        "    start=time.time()\n",
        "\n",
        "    pol = tfp.distributions.Bernoulli(logits=sentence)\n",
        "    act = pol.sample()\n",
        "    batch_actions[idx] = [ac for ac in act]\n",
        "    log_probs = [pol.log_prob(ac) for ac in act]\n",
        "    lpkeep = pol.log_prob([0])\n",
        "    lpdel = pol.log_prob([1])\n",
        "\n",
        "    onect=0\n",
        "    zeroct=0\n",
        "    lp_list = []\n",
        "    for i,ac in enumerate(act):\n",
        "      if int(ac) == 1:\n",
        "        lp_list.append(lpdel[i])\n",
        "        onect+=1\n",
        "      else:\n",
        "        lp_list.append(lpkeep[i])\n",
        "        zeroct+=1\n",
        "    # print(onect,\"/\",zeroct)\n",
        "    log_prob_sums[idx] = sum(lp_list)\n",
        "    x = str(X[idx].numpy(),'utf-8')\n",
        "    acts = batch_actions[idx]\n",
        "    tmp_str = string_ops.regex_replace(x,DEFAULT_STRIP_REGEX,'')\n",
        "    tmp_str = str(tmp_str.numpy(), 'utf-8').strip()\n",
        "    tmp_list = [i for i in tmp_str.split(' ') if i]\n",
        "    if (len(tmp_list) > len(acts)):\n",
        "      print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      sys.exit()\n",
        "    new_list = []\n",
        "    addct=0\n",
        "    rem=0\n",
        "    for i,toke in enumerate(tmp_list):\n",
        "      if acts[i] != 1:\n",
        "        new_list.append(toke)\n",
        "      else: rem+=1\n",
        "    words_removed.append(rem)\n",
        "    new_X.append(' '.join(new_list))\n",
        "    if idx==0: \n",
        "      print(\"\\tsentence action and word-removal time : \",time.time()-start)\n",
        "      # print(X[0])\n",
        "      # print(new_X[0])\n",
        "  return batch_logits, new_X, log_prob_sums, np.mean(words_removed)\n",
        "\n",
        "# from tensorflow.compat.v1.distributions import Multinomial as multinomial\n",
        "'''\n",
        "tf.random.categorical(logits=np.array([[.1,.2,.9]]),num_samples=1)\n",
        "'''\n",
        "\n",
        "import sys\n",
        "import time\n",
        "# \n",
        "# policy gradient (REINFORCE) training loop with LSTM model and BERT sentiment classifier\n",
        "# classifier must be compiled!\n",
        "# \n",
        "def train_vpg_mlp(mlp, optimizer, classifier, training_set, validation_set, epochs=5):\n",
        "  total_start=time.time()\n",
        "  stepct=0\n",
        "  # with tf.device('/device:GPU:0'):\n",
        "  for e in range(epochs):\n",
        "    epoch_losses=[]\n",
        "    counter=0\n",
        "    trainlen = len(training_set)\n",
        "    epoch_avg_del=[]\n",
        "    for idx,ex_lab in enumerate(training_set):\n",
        "    # for idx,ex_lab in enumerate(training_set.take(100)):\n",
        "      starttime=time.time()\n",
        "      example, label = ex_lab[0], ex_lab[1]\n",
        "      counter+=1\n",
        "      print('epoch={}\\tbatch={}'.format(e,counter))\n",
        "      stepct+=1\n",
        "      # run through the lstm\n",
        "      avg_words_del = []\n",
        "      logits, new_example, logprob_sums = None, None, None\n",
        "      with tf.GradientTape() as t:\n",
        "        start = time.time()\n",
        "        # logits, new_example, logprob_sums = step(lstm, example)\n",
        "        # logits, new_example, logprob_sums = step2(lstm, example)\n",
        "        logits, new_example, logprob_sums, deleted = mlpstep2(mlp, example)\n",
        "        avg_words_del.append(deleted)\n",
        "        print('step time ', time.time()-starttime)\n",
        "        # logging for curiosity\n",
        "        if e==0 and idx==0:\n",
        "          print('step time for one batch: {:.4f}'.format(time.time()-start))\n",
        "\n",
        "        # get BERT evaluations on these\n",
        "        start=time.time()\n",
        "        new_outs = classifier(tf.constant(new_example))\n",
        "        # get bert evaluation on full sentences\n",
        "        baseline_outs = classifier(example)\n",
        "\n",
        "        if e==0 and idx==0:\n",
        "          print('BERT processing time for one batch : {:.4f}'.format(time.time()-start))\n",
        "\n",
        "        # reward LSTM with the improvement over the original sentences\n",
        "        baseline_loss = tf.keras.losses.mean_squared_error(baseline_outs,tf.cast(label,float))\n",
        "        loss_object = tf.keras.losses.mean_squared_error(new_outs,tf.cast(label,float))\n",
        "        # loss_object = tf.keras.losses.binary_crossentropy(new_outs,tf.cast(label,float))\n",
        "        adjusted_loss = loss_object-baseline_loss\n",
        "        # print(\"loss_x' - loss_base : \",np.array(adjusted_loss))\n",
        "        epoch_losses.append(adjusted_loss.numpy())\n",
        "\n",
        "        # use BERT's loss for update:\n",
        "        # use the negative losses to weight the log_probs\n",
        "        weighted_logprobs = -(adjusted_loss)*logprob_sums\n",
        "        # end with tf.GradientTape()\n",
        "\n",
        "      # train bert by fitting on new_examples\n",
        "      # this way, BERT and the LSTM train together\n",
        "      with tf.device('/device:GPU:0'):\n",
        "        history = classifier.fit(x=tf.constant(new_example),\n",
        "                                  y=label,\n",
        "                                  validation_split=.2,\n",
        "                                  batch_size=len(new_example),\n",
        "                                  epochs=2,\n",
        "                                  verbose=(idx%10==0)\n",
        "                                  )\n",
        "      # print(\"BERT training accuracy : \", np.mean(history.history['binary_accuracy']))\n",
        "\n",
        "      # get gradients and apply update\n",
        "      gradients = t.gradient(weighted_logprobs, mlp.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients,mlp.trainable_variables))\n",
        "\n",
        "      epoch_avg_del.append(np.mean(avg_words_del))\n",
        "\n",
        "      if idx == 0 or idx%50==0: print('batch time {:.2f}'.format(time.time()-starttime))\n",
        "      # end for\n",
        "    print('\\n_____________________________')\n",
        "    print('[{}]\\t avg training loss : {}'.format(e,np.mean(epoch_losses)))\n",
        "    # print('[{}]\\t total train inloss : {}'.format(e,np.sum(epoch_losses)))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      val_loss, val_acc = classifier.evaluate(validation_set)\n",
        "    print(\"[{}] BERT validation loss     :\\t{}\\n\\t\\tvalidation accuracy :\\t{}\".format(idx,val_loss, val_acc))\n",
        "    print(epoch_avg_del)\n",
        "  print('DONE. {:.4f}'.format(time.time()-total_start))\n",
        "\n",
        "def remove_words_mlp(model, X):\n",
        "  minibatch=[]\n",
        "  batch_logits = model(X)\n",
        "  batch_actions =[[] for _ in batch_logits]\n",
        "  log_prob_sums =[0.0 for _ in batch_logits]\n",
        "  _log_prob_sums =[None for _ in batch_logits]\n",
        "  new_X = []\n",
        "  removed_list=[]\n",
        "  for idx,sentence in enumerate(batch_logits):\n",
        "    removed=0\n",
        "    temp_log_probs = []\n",
        "    start=time.time()\n",
        "\n",
        "    pol = tfp.distributions.Bernoulli(logits=sentence)\n",
        "    act = pol.sample()\n",
        "    batch_actions[idx] = [ac for ac in act]\n",
        "    # print(onect,\"/\",zeroct)\n",
        "    x = str(X[idx].numpy(),'utf-8')\n",
        "    acts = batch_actions[idx]\n",
        "    tmp_str = string_ops.regex_replace(x,DEFAULT_STRIP_REGEX,'')\n",
        "    tmp_str = str(tmp_str.numpy(), 'utf-8').strip()\n",
        "    tmp_list = [i for i in tmp_str.split(' ') if i]\n",
        "    if (len(tmp_list) > len(acts)):\n",
        "      print(\"ERROR: REGEX INSUFFICIENT\")\n",
        "      sys.exit()\n",
        "    new_list = []\n",
        "    addct=0\n",
        "    for i,toke in enumerate(tmp_list):\n",
        "      if acts[i] != 1:\n",
        "        new_list.append(toke)\n",
        "      else: removed+=1\n",
        "    removed_list.append((removed,len(tmp_list)))\n",
        "    new_X.append(' '.join(new_list))\n",
        "    if idx==0: \n",
        "      print(\"\\tsentence action and word-removal time : \",time.time()-start)\n",
        "\n",
        "  return new_X, removed_list\n",
        "\n",
        "def evaluate_mlp_model(mlp, classifier, testing_set):\n",
        "  test_x=[]\n",
        "  test_y=[]\n",
        "  total=len(testing_set)\n",
        "  stats = []\n",
        "  for idx,example in enumerate(testing_set):\n",
        "    test_y.append(example[1].numpy())\n",
        "    new_x, _stats = remove_words_mlp(mlp,example[0])\n",
        "    stats.append(_stats) \n",
        "    test_x.append(np.array(new_x))\n",
        "    print('MLP processing batch',idx,'/',total-1)\n",
        "    if idx==0:\n",
        "      print(\"Sample MLP word removal\")\n",
        "      print(example[0][:5])\n",
        "      print(new_x[:5])\n",
        "    # print(new_x)\n",
        "    # print(len(stats))\n",
        "    # sys.exit()\n",
        "  test_loss, test_acc = classifier.evaluate(x=test_x,y=test_y)\n",
        "  return test_loss, test_acc, stats"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P29QUZQMW-Ck"
      },
      "source": [
        "mlp_classifier = build_bert_model()\n",
        "compile_bert_model(model=mlp_classifier, training_set=train_ds)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "z1nGSuChF1xR",
        "outputId": "0740eae0-8396-4689-ca97-555de162ef07"
      },
      "source": [
        "mlp_classifier.fit(x=train_ds.shard(20,1))\n",
        "train_vpg_mlp(mlp_model,\n",
        "              optimizer=mlp_optim,\n",
        "              classifier=mlp_classifiershard(10,1),\n",
        "              training_set=train_ds,\n",
        "              validation_set=val_ds,\n",
        "              epochs=1)\n",
        "loss, acc, stat = evaluate_mlp_model(mlp=mlp_model, classifier=mlp_classifier)\n",
        "means=[]\n",
        "for batch in stat:\n",
        "  batch_means=[]\n",
        "  for pair in batch:\n",
        "    batch_means.append(pair[0])\n",
        "  means.append(np.mean(batch_means))\n",
        "print('avg words removed: ',np.mean(means))\n",
        "# print(\"avg removed words: \", np.mean(stat[:][0]))\n",
        "print(\"eval acc: \", acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "312/312 [==============================] - 2373s 8s/step - loss: 0.5101 - binary_accuracy: 0.7340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-935d61ae9557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_vpg_mlp(mlp_model,\n\u001b[0;32m----> 3\u001b[0;31m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mtraining_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B28ppFe-fW_0",
        "outputId": "345dea59-e7b0-46e4-b446-905b9fc0f55b"
      },
      "source": [
        "loss, acc, stats = evaluate_mlp_model(mlp=mlp_model,\n",
        "                                      classifier=mlp_classifier,\n",
        "                                      testing_set=test_ds.shard(690,0)\n",
        ")\n",
        "means=[]\n",
        "for batch in stats:\n",
        "  batch_means=[]\n",
        "  for pair in batch:\n",
        "    batch_means.append(pair[0])\n",
        "  means.append(np.mean(batch_means))\n",
        "print('avg words removed: ',np.mean(means))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tsentence action and word-removal time :  0.07212686538696289\n",
            "MLP processing batch 0 / 2\n",
            "Sample MLP word removal\n",
            "tf.Tensor(\n",
            "[b'If one were to return to the dawn of the talking picture, one would prophesy a bright future for Harold Lloyd. Unlike his competitors, he was comedic actor trained on the legitimate stage not a performed raised in the purgatory of the music hall or vaudeville circuit. He had a good voice which matched his image. Moreover, from 1924 on, his \"silent\" films had incorporated sequences based on sound gags lost on the audience (e.g., the bell sequence at the Fall Frolic from THE FRESHMAN and the monkey sequence in THE KID BROTHER). Yet Lloyd\\'s sound features consistently failed at the box office once the novelty of WELCOME DANGER had ebbed. Lloyd blamed his fall on many external sources, but never realized that the Glass character\\'s enemy was not sound but the Great Depression. Pre-Depression audiences, giddy with optimism, may have rooted for this ambitious go-getter in whom they saw their surrogate; Depression audiences despised him as the person likely to foreclose on their mortgage and throw them in the gutter. Compounding this problem of character choice is Lloyd\\'s perception as an insincere glad hander. Sincerity, of course, is a subjective appraisal, but it is undeniable that Lloyd, despite his own tragic upbringing, could never play a convincing down-and-outer. Perhaps this is because he feared returning to that state permanently. THE CAT\\'S-PAW fails for these reasons, but it alone suffers from the revelation of Lloyd\\'s pro-fascist agenda. Many film scholars believe that Lloyd was prompted to make this film because he saw the presidency of FDR as a dictatorship bent on soaking the rich and soft on crime. We should remember that he was not alone in this feeling. DeMille had directed THIS DAY AND AGE, a pro-police state drama, the previous year. We should also remember that America was founded by hotheaded tax protesters and continues to be motivated by those who want something without paying for it. TCP suffered because it treated fascism lightly in a \"comedy\" and because its release was particularly ill-timed given the events in Germany in that year. The Production Code of 1934 would ultimately curtail the glorification of vigilante justice and reaffirm the rule of constitutional law, cumbersome as it might be. The ideal of the benevolent despot, the good-intentioned all-powerful leader who brings about a utopia once freed of the checks and balances on this omnipotence, dates to classical antiquity. For this reason, totalitarian regimes fear laughter even though it acts as a safety valve. Ironically, the mere existence of TCP, a film which demonizes the democratic experience of the country of its origin, shows that FDR\\'s America was secure enough to accept criticism. One sees no parallel criticism in Hitler\\'s Germany, Stalin\\'s USSR, or Mussolini\\'s Italy. But can one laugh at the gallows humor of pending fascism? Lloyd\\'s unnuanced film is skewed to the right and might have been written by Dr Goebbels himself if he\\'d had a sense of humor, of course. It posits an alternative history in which a chosen one restores order and lost honor BY ANY MEANS NECESSARY, and does so with good nature and fun. Impending fascism approached by the left is, of course, Chaplin\\'s THE GREAT DICTATOR. This latter film has the benefit of being set in another country and based on a thinly veiled actual persona and events. THE GREAT DICTATOR produces few laughs today because it under-estimated the extent of human evil, but it succeeds despite its artless and inappropriate speechifying, because it has the distinct advantage of being vindicated by history. Lloyd, however, should be credited for two things: first, he neither made any further pro-fascist films nor produced any subsequently hypocritically pro-allied films during the War: second, he never sold TCP to television. The post-1945 world had seen the face of fascism and it wasn\\'t amusing.'\n",
            " b'A beautifully photographed and paced short film. It evocatively captures the feeling of this family and much of the country during the period just prior to and after Pearl Harbor.<br /><br />I appreciated the visual look of the film -- naturalistic and simultaneously poetic. Great work by a great D.P., David Boyd.<br /><br />Though a family film, the story never becomes maudlin or saccharin. We understand and believe the motivation that propels the young boy on his odyssey. I understand the love of the younger brother for his older brother and do not question why he sets out to do what he does. I understand that he is driven by a deep desire to be with his brother in this time of crisis. The kid is tough, and the performance by Jonathan Furr is superb as is the veteran performance by Ron Perlman.'\n",
            " b\"Shameless waste of my time as a viewer. This is one of the worst films I've seen in ages. Please do not rent it as you will regret doing so! Guaranteed! I wonder how Kathleen Turner ended up in this! She is a legitimate actress and people would perhaps be attracted to this film because of her. But it really is better to act as if this title was never made! It should not have come into existence!\"\n",
            " b'\"Gargle with old razor blades. Can I help it if I\\'m not cousin Basil? I think the piano\\'s out of tune. Ginger Grey. This is your little snookums.\" Laughs throughout the entire 20 minute short as the boys spoof gold diggers and opera singers. They even manage to show us how to properly demonstrate to some attractive ladies how to handle both a rifle and a bear trap. Wonder how many times they rehearsed the scene with the phone booth. Adding Christine McIntyre and Emil Sitka, 2 frequent collaborators, to the mix makes it even better. Only Vernon Dent is missing. The Stooges did some great individual scenes, but this was their best overall.'\n",
            " b'Crash is overwrought, over-thought and over-baked. A great example of how to make a pompous and self-important film with a message. Haggis tries too hard to make his point and overreaches in just about every category of the film. It feels very much in love with its own sense of social relevance and has all the subtlety of a jackhammer to the skull. Sure, race relations affect everyone and there\\'s a great deal of ambiguity to the issue, but the universe of \\'Crash\\' operates to suggest that we are all victims to our own perceptions on race. It\\'s a tiresome thread that repeats itself ad nauseum and wears out its welcome within the first 30 minutes. I found the outcomes of the characters unsurprising and forced and the film took forever to sputter out and die. The fact that this film is in the top 50 movies on IMDb is a testament to the public\\'s willingness to get suckered by Hollywood malarkey.<br /><br />Indeed, if you want the real \"Crash,\" go check out the one by David Cronenberg: dark, twisted and original. This film plays as preachy, tiresome, and masochistic.'], shape=(5,), dtype=string)\n",
            "['were to return to the talking picture one would a bright Harold Lloyd his competitors actor the in the of the music hall or vaudeville circuit had good voice which matched from on his silent films on eg the at Frolic from THE FRESHMAN the sequence in THE BROTHER Yet failed at the box office the novelty WELCOME DANGER fall sources but never realized that the Glass characters not the PreDepression giddy with may have ambitious in their surrogate audiences him as the person foreclose on and gutter problem choice Lloyds perception Sincerity course is own tragic could never convincing downandouter Perhaps this because to that state CATSPAW fails for reasons but alone suffers from film scholars this saw as bent soaking the rich and on should that he was not this feeling had directed AND AGE drama previous year should also remember that America was founded by and continues to be motivated by those want without for it TCP it fascism lightly comedy its was particularly the events Germany year The of would ultimately the of vigilante the rule of as it might be of despot who utopia once of checks on this omnipotence to classical antiquity this reason fear even acts as Ironically the mere a democratic experience of country of its origin that FDRs America enough to sees criticism Germany Stalins USSR or Mussolinis Italy one at gallows humor pending fascism Lloyds unnuanced is to right might been if hed a sense of humor course It posits in chosen one order and honor MEANS with good nature Impending approached the left is This latter has being and veiled and events THE today because the human its because it the distinct advantage by history Lloyd however should credited for two first any subsequently hypocritically during second he never The had the fascism wasnt amusing', 'A beautifully photographed paced short evocatively captures the of this and the during just prior and Pearl Harborbr I the visual look of naturalistic and simultaneously poetic Great work a DP Boydbr Though family the never becomes understand believe that propels on his odyssey I the love of the his older question he sets out do he does I that he is driven by a deep desire with brother time of crisis kid is the Jonathan Furr as is veteran by Ron', 'of my as a viewer This is of Ive seen ages do rent it as will regret doing wonder Turner in She actress and be because of But really is act was never made not have existence', 'old razor Can it if cousin Basil think pianos tune Ginger This is the entire 20 diggers opera They even manage show to to ladies how both a bear trap Wonder many times they with McIntyre and Sitka 2 frequent collaborators to even better Dent is missing great scenes but this was their overall', 'is great example of how make and film a message tries too hard to make his point and overreaches just category of the film It very love its social relevance and has subtlety skull Sure affect everyone theres deal of the the of Crash suggest victims own Its a itself nauseum and the first minutes the of to sputter The that in the top 50 IMDb a to willingness to by Hollywood Indeed you want the check out the Cronenberg dark film plays']\n",
            "\tsentence action and word-removal time :  0.05637502670288086\n",
            "MLP processing batch 1 / 2\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6512 - binary_accuracy: 0.6562\n",
            "avg words removed:  120.28125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}